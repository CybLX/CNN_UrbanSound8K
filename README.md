# ğŸ”Š Urban Sound Classifier - Deep Learning with PyTorch

This project implements a full pipeline for preprocessing, modeling, and metric visualization for **urban sound classification** using **PyTorch** and the **UrbanSound8K** dataset. The pipeline handles audio files, applies `data augmentation` techniques, and converts data into MEL spectrograms ready to feed into a CNN.

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ checkpoint/                     # Checkpoints with models, metrics, scheduler and optimizer
â”‚   â””â”€â”€ train_and_val_metrics.png   # Plot of accuracy and loss
â”œâ”€â”€ data analysis/                  # Notebook or scripts with data preprocessing analysis
â”œâ”€â”€ src/                            # Source code
â”‚   â”œâ”€â”€ inference.py                # Inference routine for trained models
â”‚   â”œâ”€â”€ model.py                    # CNN architecture
â”‚   â”œâ”€â”€ training.py                 # Training loop, validation, early stopping
â”‚   â””â”€â”€ utils.py                    # Dataset class, preprocessing functions
â”œâ”€â”€ UrbanSound8K/                   # Dataset folder
â”œâ”€â”€ ForPrediction.py                # Script for inference on new audio files
â”œâ”€â”€ UrbanSound_Training.py          # Main training routine
â””â”€â”€ README.md
```

---

## ğŸ“š Objective

The goal of this project is to develop a classifier for **urban sounds** using **convolutional neural networks (CNNs)** with **PyTorch**. Sounds are extracted from the **UrbanSound8K** dataset, and the system can identify noise such as car horns, dog barks, sirens, and more, based on MEL spectrograms. The pipeline is complete: from raw audio loading to CNN training and results visualization.

---

## ğŸ”„ Preprocessing Pipeline

- ğŸµ Reading `.wav` files using `torchaudio`
- ğŸ” Transformation into **MelSpectrogram** with configurable parameters
- ğŸ§ª Application of **SpecAugment** (time and frequency masking)
- ğŸ”¢ Normalization of spectrogram data
- ğŸ·ï¸ Conversion to tensors and label pairing

Each sample is standardized to a fixed input size, making CNN training consistent.

---

## ğŸ“¦ Custom Dataset

Custom dataset based on `torch.utils.data.Dataset` and `UrbanSound8K`:

- Reads from `metadata/UrbanSound8K.csv`
- Uses the `fold` column to split training and validation sets
- Lazy loading of `.wav` files
- Spectrogram normalization and caching
- Conditional `data augmentation` only during training

---

## ğŸ‹ï¸ Training

Model training is handled by the `Trainer` class, which includes:

- âœ… Support for **early stopping** and automatic **checkpoints**
- ğŸ“‰ Calculation of metrics like loss, accuracy, recall, and F1
- ğŸ“ Logs saved in `.json` formats
- ğŸ“Š Automatic plotting of training curves (loss and accuracy)
- ğŸ§ª Validation at the end of each epoch

CNN architecture includes:

- ğŸ”¹ 4 convolutional blocks with `BatchNorm`, `ReLU`, `Dropout`
- ğŸ”¹ `MaxPooling` between blocks
- ğŸ”¹ `Flatten` + fully connected layers
- ğŸ”¹ Final `Softmax` layer for 10-class classification

---

## ğŸ“Š Metrics Visualization

Automatic visualizations after training:

- Metric logs saved per epoch as CSV files
- Visualization script in `utils/visualization.py`
- Charts for:
  - ğŸ¯ Accuracy and loss per epoch
  - ğŸ”„ Execution time per epoch

---

## ğŸ¼ Dataset

Using the **UrbanSound8K** dataset:

- ğŸ”Š **8732 audio files** (`.wav`)
- ğŸ·ï¸ **10 classes of urban sounds** (e.g., siren, bark, car horn)
- ğŸ“ **Split into 10 folders** (`fold1` to `fold10`)
- ğŸ—‚ï¸ Metadata file `metadata/UrbanSound8K.csv` contains:
  - `slice_file_name`
  - `fold`
  - `classID`

ğŸ”— **Download link**: [UrbanSound8K](https://urbansounddataset.weebly.com/urbansound8k.html)

---

## ğŸ“ Requirements

Install the requirements with:

```bash
pip install -r requirements.txt
```

Key libraries:
- torch
- torchaudio
- scikit-learn
- matplotlib
- tqdm
- pandas
- numpy

---

## ğŸ“Œ References

- [UrbanSound8K Dataset](https://urbansounddataset.weebly.com/urbansound8k.html)  
- [SpecAugment: Data Augmentation for ASR](https://arxiv.org/abs/1904.08779)  
- [PyTorch](https://pytorch.org/)  
- [Torchaudio Docs](https://pytorch.org/audio/stable/index.html)  
- [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)  
- [Audio Deep Learning Made Simple](https://medium.com/data-science/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5) - Ketan Doshi

---

## ğŸ‘¨â€ğŸ’» Author

Developed by **Lucas Alves**  
ğŸ“§ Email: [alves_lucasoliveira@usp.br](mailto:alves_lucasoliveira@usp.br)  
ğŸ™ GitHub: [cyblx](https://github.com/cyblx)  
ğŸ’¼ LinkedIn: [cyblx](https://www.linkedin.com/in/cyblx)
